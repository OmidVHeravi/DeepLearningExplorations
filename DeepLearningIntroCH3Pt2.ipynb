{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrQEj9PrEhishEKx5apGKW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Autoregressive Models"
      ],
      "metadata": {
        "id": "8QKU7ASbBh7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain Rule"
      ],
      "metadata": {
        "id": "Y4_OyVfgBj8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us recall the chain rule in the context of probability theory first;\n",
        "\n",
        "$$\n",
        "P(X_1 = x_1, X_2 = x_2, X_3 = x_3,..., X_n = x_n) = \\\\\n",
        "P(X_1 = x_1) \\\\\n",
        " \\times P(X_2 = x_2 | X_1 = x-1)\n",
        " \\\\ ... \\\\\n",
        " \\times P(X_n = x_n | X_1 = x_1, ..., X_{n-1} = x_{n-1})\n",
        "$$\n",
        "\n",
        "* This rule becomes far more useful in the case of random variables with finite dimensional range. For instance, in NLP,  a sentence can be viewed as a sequence of words (or tokens). Each word in the sentence could be represented as a random variable in a sequence, and the joint probability of the entire sentence could be represented as a product of conditional probabilities of each word given the previous words (using the chain rule).\n",
        "\n",
        "* This representation is particularly efficient because it simplifies the task of estimating these joint probabilities. If we had to estimate the joint probability of an entire sentence as a whole, we would need to estimate a very large number of probabilities (since there are so many possible sentences). However, by breaking the problem down into a sequence of conditional probabilities, we greatly reduce the number of probabilities that need to be estimated. This is especially useful when the number of possible \"tokens\" (i.e., words or other units) is finite and relatively small, as it often is in applications like NLP or image analysis.\n",
        "\n",
        "* In sum, the chain rule is especially useful when we can break down a complex signal into a sequence of simpler, discrete elements, and estimate the probabilities of these elements in sequence."
      ],
      "metadata": {
        "id": "JgJCbhnABofe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yItUYevjBjeC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}