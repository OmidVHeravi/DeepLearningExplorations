{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDlJmSM7MYHMjxEtej+I4e"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Autoregressive Models"
      ],
      "metadata": {
        "id": "8QKU7ASbBh7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain Rule"
      ],
      "metadata": {
        "id": "Y4_OyVfgBj8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us recall the chain rule in the context of probability theory first;\n",
        "\n",
        "$$\n",
        "P(X_1 = x_1, X_2 = x_2, X_3 = x_3,..., X_n = x_n) = \\\\\n",
        "P(X_1 = x_1) \\\\\n",
        " \\times P(X_2 = x_2 | X_1 = x-1)\n",
        " \\\\ ... \\\\\n",
        " \\times P(X_n = x_n | X_1 = x_1, ..., X_{n-1} = x_{n-1})\n",
        "$$\n",
        "\n",
        "* This rule becomes far more useful in the case of random variables with finite dimensional range. For instance, in NLP,  a sentence can be viewed as a sequence of words (or tokens). Each word in the sentence could be represented as a random variable in a sequence, and the joint probability of the entire sentence could be represented as a product of conditional probabilities of each word given the previous words (using the chain rule).\n",
        "\n",
        "* This representation is particularly efficient because it simplifies the task of estimating these joint probabilities. If we had to estimate the joint probability of an entire sentence as a whole, we would need to estimate a very large number of probabilities (since there are so many possible sentences). However, by breaking the problem down into a sequence of conditional probabilities, we greatly reduce the number of probabilities that need to be estimated. This is especially useful when the number of possible \"tokens\" (i.e., words or other units) is finite and relatively small, as it often is in applications like NLP or image analysis.\n",
        "\n",
        "* In sum, the chain rule is especially useful when we can break down a complex signal into a sequence of simpler, discrete elements, and estimate the probabilities of these elements in sequence."
      ],
      "metadata": {
        "id": "JgJCbhnABofe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus given some model, we have the following relation $$F(x_1,...,x_n,w)= log(\\hat{P}(X_n | X_1 = x_1,...,X_n = x_n))$$\n",
        "\n",
        "and using the chain rule above, sampling an entire sentence of length $N$ amounts to sampling each $x_i$ one after another conditioned on joint probability distribution of the tokens thus far i.e. the predicted posterior distributiuon. This is known as an a***utoregressive generative*** model."
      ],
      "metadata": {
        "id": "oHp3n9ZuRGCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Causal Models"
      ],
      "metadata": {
        "id": "bUhVZUYGR6jI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1sxu8QElR8nI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yItUYevjBjeC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}