{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJFVXm6FGLuq/jZtuuRr9B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent"
      ],
      "metadata": {
        "id": "qnSwiW1UQqfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Given that in most cases our trainable parameters aka the weights are not represented in math closed forms, hence in order to minimize/maximize the values of the weights, one must resort to a process known as ***Gradient Descent***.\n",
        "\n",
        "* Gradient Descent is the process of choosing some random inital weight, i.e. $w_0$, and we intend to improve the value of this weight in the direction in which it minimizes our loss, thus we iteratively improve the weight by computing the gradient/derivative of the loss/error function with respect to the trainable parameters/weight. The exact expression is:\n",
        "$$\n",
        "w_{n+1} = w_n - \\eta \\times ∇Λ_w(w_n)\n",
        "$$\n",
        "\n",
        "* This is essentially the process by which we move the value of the weights in the direction in which the error is minimized as most as possible."
      ],
      "metadata": {
        "id": "Pumn0XGfQycL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Rate"
      ],
      "metadata": {
        "id": "IYw9Zca9TZFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The variable, also often the parameter $\\eta$, is known as the learning parameter. It ranges from $0$ to $1$, and it modulates the amount by which we move in the direction of the gradient.\n",
        "\n",
        "* Choosing a value too small may run the risk of the optimization getting stuck in a local minima and slowing down the optimization, and choosing a value too large will force the model to constantly get close and bounce around a good minima but never attain it."
      ],
      "metadata": {
        "id": "ASjcm_FLTcmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "Tk6nYucgWHnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bajzBHdEWKZs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bTdhrJ0sQs1h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}