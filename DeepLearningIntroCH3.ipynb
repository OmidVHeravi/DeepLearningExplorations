{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLfKvcpt89W93PZ8NS22rD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "26r437QI7EAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whilst training a machine learning model, the most crucial step occurs when we make the choise of which loss function to use. Recall that a loss function, $Λ(w)$, which measures the amount of error our due to our model, $F( Δ, w)$, is producing on some training set $Δ$. Hence the loss function plays a crucial role in the success of our model, moreover the most important step in training also occurs how we choose to minimize this loss funciton, namely how we wish to optimize the performance of our model."
      ],
      "metadata": {
        "id": "cidntlKc7I4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Functions"
      ],
      "metadata": {
        "id": "IGxFzGey8LAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Mean Squared Error: $$\n",
        "Λ(w) = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - F(x_i, w))^2\n",
        "$$\n",
        "is the most common loss function for prediction of continious values."
      ],
      "metadata": {
        "id": "QFQ8-y7P-30b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desnity Modeling and Likelihood of Data"
      ],
      "metadata": {
        "id": "Nnq3EhaO_UIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The \"likelihood of the data\" refers to the probability of observing the given data given the parameters of the model. In other words, given a model with certain parameters, how likely is it that we would observe the data that we have?\n",
        "\n",
        "*  In density modeling, the standard loss is often the negative log-likelihood of the data.\n",
        "\n",
        "* We use the negative log-likelihood because likelihoods can be very small numbers, so taking the logarithm helps with numerical stability, and we take the negative because we want to minimize our loss function, but likelihoods should be maximized."
      ],
      "metadata": {
        "id": "1PgK7M205QdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example; Let's say we have a model with parameters $\\theta$, and a dataset $\\mathbf{X} = {x_1, x_2, ..., x_n}$. The log-likelihood of the data is given by:\n",
        "$$\n",
        "\\log p(\\mathbf{X} | \\theta) = \\sum_{i=1}^{n} \\log p(x_i | \\theta)\n",
        "$$\n",
        "And the loss function, which is the negative log-likelihood, is:\n",
        "$$\n",
        "-\\log p(\\mathbf{X} | \\theta) = -\\sum_{i=1}^{n} \\log p(x_i | \\theta)\n",
        "$$\n",
        "\n",
        "In training the model, we aim to find the model parameters $\\theta$ that minimize this loss. This is equivalent to maximizing the likelihood of the data, hence the term \"maximum likelihood estimation\"."
      ],
      "metadata": {
        "id": "3UyuHfJd5ezx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Entropy\n",
        "\n",
        "* For classification problems that are adressed via machine learning algorithms, the usual choice of model is one which outputs a single column vector where the length corresponds to the number of classes and the values to the probability assosicated with the input belonging to that class. Also known as the ***Logit*** vector.\n",
        "\n",
        "* Suppose $X$ is some input, $Y$ is the class which we wish to predict, we then compute using the model $F$ an estimate of the posterior probabilities; $$\\hat{P}(Y = y | X = x) = \\frac{exp(F(x,w)_y)}{\\sum_z exp(F(x,w)_z)} $$\n",
        "This is the probability of $Y=y$ conditioned on $X=x$. This is called the ***Softmax***, or also ***softargmax*** of the logits.\n",
        "\n"
      ],
      "metadata": {
        "id": "uwmXEZtGAB9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To be more precise, since we wish the model to be trained to the point in which it maximizez the probability of true classes, in other words, it must minimizez the corss entropy function, that is:\n",
        "$$\n",
        "Λ_{\\text{ce}}(w) = \\\\ \\frac{-1}{N} \\sum_{i=1}^{N} log(\\hat{P}(Y = y_n | X = x_n)) = \\\\ \\frac{1}{N} \\sum_{i=1}^{N} -log(\\frac{exp(F(x,w)_y)}{\\sum_z exp(F(x,w)_z)}) = \\\\ \\frac{1}{N} \\sum_{i=1}^{N} Λ'_{\\text{ce}}(F(x_n,w),y_n)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "u2o-rQSZzQIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contrastive Loss"
      ],
      "metadata": {
        "id": "YK-_8W-F00xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Suppose we wish to learn a distance function such that the measure of the distance of some data sample $x_a$ from a certain semantic class to any other data sample $x_b$ from that same class is smaller than the distance it would have to any other data sample say $x_c$ which is not in the same semantic class. This process is called ***metric learning***.  \n",
        "\n",
        "* To achieve this, most often the form of loss function which is utilized is called a ***constrastive loss*** function.\n",
        "\n"
      ],
      "metadata": {
        "id": "MZbmHxXs04AS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of contrastive loss:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(x, x^{+}, x^{-}) = \\frac{1}{2N} \\sum_{i=1}^{N} [d(f(x_i), f(x_i^{+}))^{2} + \\max(0, \\text{margin} - d(f(x_i), f(x_i^{-})))^{2}]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "* $x$ is an anchor sample\n",
        "* $x^{+}$ is a positive sample\n",
        "* $x^{-}$ is a negative sample\n",
        "* $f()$ is an embedding function\n",
        "* $d()$ is a distance function (e.g. euclidean distance)\n",
        "* $\\text{margin}$ is a margin parameter\n",
        "* $N$ is the number of sample pairs.\n",
        "\n",
        "\n",
        "The contrastive loss aims to make the distance between an anchor and positive sample small, while making the distance between the anchor and negative sample greater than the margin $\\text{margin}$. It helps learn useful embeddings where similar samples are close and dissimilar samples are far apart in the embedding space."
      ],
      "metadata": {
        "id": "6ufXcndNILaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Engineering the Loss"
      ],
      "metadata": {
        "id": "eJPsrffgJJZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note that it is also possible to add specific terms to the loss function which depend on certain trainable parameters of the model so that certain outcomes are favorable.\n",
        "\n",
        "* For one example consider the weight decay regulariation technique in which case adds an additional term to the loss function that penalizes large weights. This helps prevent overfitting by encouraging smaller, more diffuse weight values that generalize better.\n",
        "\n",
        "* The L2 norm regularization has the effect of pulling weights towards the origin. The strength of this pull is controlled by the hyperparameter $\\lambda$. With a larger $\\lambda$, the regularization effect is stronger.\n",
        "\n",
        "* So in summary, weight decay regularization helps prevent overfitting and improves generalization performance of neural network models. The penalty on large weights encourages diffuse weights suitable for generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "iERplXBIJNmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an mathematical formulation of weight decay regularization:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w; x, y) = \\ell(f(x;w), y) + \\frac{\\lambda}{2}\\lVert w \\rVert_2^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "$\\ell$ is the loss function (e.g. cross-entropy loss)\n",
        "$f(x;w)$ is the model with weights $w$\n",
        "$(x, y)$ is the training sample and label\n",
        "$\\lambda$ is the regularization strength hyperparameter\n",
        "$\\lVert w \\rVert_2^2$ is the L2 norm squared (sum of squared weights)"
      ],
      "metadata": {
        "id": "vZ72oge-VuU9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDedx7FoJMQy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}