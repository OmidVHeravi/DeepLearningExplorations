{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTPSLj6m7ms3AyRizNXPt0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "26r437QI7EAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whilst training a machine learning model, the most crucial step occurs when we make the choise of which loss function to use. Recall that a loss function, $Λ(w)$, which measures the amount of error our due to our model, $F( Δ, w)$, is producing on some training set $Δ$. Hence the loss function plays a crucial role in the success of our model, moreover the most important step in training also occurs how we choose to minimize this loss funciton, namely how we wish to optimize the performance of our model."
      ],
      "metadata": {
        "id": "cidntlKc7I4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Functions"
      ],
      "metadata": {
        "id": "IGxFzGey8LAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Mean Squared Error: $$\n",
        "Λ(w) = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - F(x_i, w))^2\n",
        "$$\n",
        "is the most common loss function for prediction of continious values."
      ],
      "metadata": {
        "id": "QFQ8-y7P-30b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desnity Modeling and Likelihood of Data"
      ],
      "metadata": {
        "id": "Nnq3EhaO_UIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The \"likelihood of the data\" refers to the probability of observing the given data given the parameters of the model. In other words, given a model with certain parameters, how likely is it that we would observe the data that we have?\n",
        "\n",
        "*  In density modeling, the standard loss is often the negative log-likelihood of the data.\n",
        "\n",
        "* We use the negative log-likelihood because likelihoods can be very small numbers, so taking the logarithm helps with numerical stability, and we take the negative because we want to minimize our loss function, but likelihoods should be maximized."
      ],
      "metadata": {
        "id": "1PgK7M205QdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example; Let's say we have a model with parameters $\\theta$, and a dataset $\\mathbf{X} = {x_1, x_2, ..., x_n}$. The log-likelihood of the data is given by:\n",
        "$$\n",
        "\\log p(\\mathbf{X} | \\theta) = \\sum_{i=1}^{n} \\log p(x_i | \\theta)\n",
        "$$\n",
        "And the loss function, which is the negative log-likelihood, is:\n",
        "$$\n",
        "-\\log p(\\mathbf{X} | \\theta) = -\\sum_{i=1}^{n} \\log p(x_i | \\theta)\n",
        "$$\n",
        "\n",
        "In training the model, we aim to find the model parameters $\\theta$ that minimize this loss. This is equivalent to maximizing the likelihood of the data, hence the term \"maximum likelihood estimation\"."
      ],
      "metadata": {
        "id": "3UyuHfJd5ezx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Entropy\n",
        "\n",
        "* For classification problems that are adressed via machine learning algorithms, the usual choice of model is one which outputs a single column vector where the length corresponds to the number of classes and the values to the probability assosicated with the input belonging to that class. Also known as the ***Logit*** vector.\n",
        "\n",
        "* Suppose $X$ is some input, $Y$ is the class which we wish to predict, we then compute using the model $F$ an estimate of the posterior probabilities; $$\\hat{P}(Y = y | X = x) = \\frac{exp(F(x,w)_y)}{\\sum_z exp(F(x,w)_z)} $$\n",
        "This is the probability of $Y=y$ conditioned on $X=x$. This is called the ***Softmax***, or also ***softargmax*** of the logits.\n",
        "\n"
      ],
      "metadata": {
        "id": "uwmXEZtGAB9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To be more precise, since we wish the model to be trained to the point in which it maximizez the probability of true classes, in other words, it must minimizez the corss entropy function, that is:\n",
        "$$\n",
        "Λ_{\\text{ce}}(w) = \\\\ \\frac{-1}{N} \\sum_{i=1}^{N} log(\\hat{P}(Y = y_n | X = x_n)) = \\\\ \\frac{1}{N} \\sum_{i=1}^{N} -log(\\frac{exp(F(x,w)_y)}{\\sum_z exp(F(x,w)_z)}) = \\\\ \\frac{1}{N} \\sum_{i=1}^{N} Λ'_{\\text{ce}}(F(x_n,w),y_n)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "u2o-rQSZzQIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contrastive Loss"
      ],
      "metadata": {
        "id": "YK-_8W-F00xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MZbmHxXs04AS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkAKwXqG7DkY"
      },
      "outputs": [],
      "source": []
    }
  ]
}